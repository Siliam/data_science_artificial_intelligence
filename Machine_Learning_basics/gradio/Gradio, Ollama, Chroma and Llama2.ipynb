{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dc352bf4-bfe2-461c-a668-aec6160a94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install pypdf\n",
    "# ! pip3 install cohere\n",
    "# ! pip3 install chromadb\n",
    "# ! pip3 install typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1304e6-81f2-463e-af8a-672a60e0ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd7b455-7b25-4c9f-8852-4603bf680495",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'openchat:latest' #You can replace the model name if needed\n",
    "context = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc3eda2-f8e5-469b-b4ee-566419ad8021",
   "metadata": {},
   "source": [
    "# Through the Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3d55ada-6bdc-40e3-9b43-73232b7278c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, context, top_k, top_p, temp):\n",
    "    r = requests.post('http://localhost:11434/api/generate',\n",
    "                     json={\n",
    "                         'model': model,\n",
    "                         'prompt': prompt,\n",
    "                         'context': context,\n",
    "                         'options':{\n",
    "                             'top_k': top_k,\n",
    "                             'temperature':top_p,\n",
    "                             'top_p': temp\n",
    "                         }\n",
    "                     },\n",
    "                     stream=False)\n",
    "    r.raise_for_status()\n",
    "\n",
    " \n",
    "    response = \"\"  \n",
    "\n",
    "    for line in r.iter_lines():\n",
    "        body = json.loads(line)\n",
    "        response_part = body.get('response', '')\n",
    "        print(response_part)\n",
    "        if 'error' in body:\n",
    "            raise Exception(body['error'])\n",
    "\n",
    "        response += response_part\n",
    "\n",
    "        if body.get('done', False):\n",
    "            context = body.get('context', [])\n",
    "            return response, context\n",
    "\n",
    "\n",
    "\n",
    "def chat(input, chat_history, top_k, top_p, temp):\n",
    "\n",
    "    chat_history = chat_history or []\n",
    "\n",
    "    global context\n",
    "    output, context = generate(input, context, top_k, top_p, temp)\n",
    "\n",
    "    chat_history.append((input, output))\n",
    "\n",
    "    return chat_history, chat_history\n",
    "  #the first history in return history, history is meant to update the \n",
    "  #chatbot widget, and the second history is meant to update the state \n",
    "  #(which is used to maintain conversation history across interactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a1e1ea-a502-4314-9f88-19ca16a80968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello\n",
      "!\n",
      " I\n",
      " am\n",
      " here\n",
      " to\n",
      " help\n",
      " you\n",
      " with\n",
      " any\n",
      " questions\n",
      " or\n",
      " information\n",
      " you\n",
      " need\n",
      ".\n",
      " Please\n",
      " feel\n",
      " free\n",
      " to\n",
      " ask\n",
      " anything\n",
      " and\n",
      " I\n",
      " will\n",
      " do\n",
      " my\n",
      " best\n",
      " to\n",
      " assist\n",
      " you\n",
      ".\n",
      "\n",
      " Here\n",
      " are\n",
      " a\n",
      " few\n",
      " notable\n",
      " Arab\n",
      " scientists\n",
      " throughout\n",
      " history\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      ".\n",
      " Al\n",
      "-\n",
      "K\n",
      "h\n",
      "aw\n",
      "ar\n",
      "iz\n",
      "mi\n",
      " (\n",
      "c\n",
      ".\n",
      " \n",
      "7\n",
      "8\n",
      "0\n",
      " -\n",
      " \n",
      "8\n",
      "5\n",
      "0\n",
      ")\n",
      " -\n",
      " A\n",
      " Pers\n",
      "ian\n",
      " scholar\n",
      " who\n",
      " made\n",
      " significant\n",
      " contributions\n",
      " to\n",
      " mathemat\n",
      "ics\n",
      ",\n",
      " particularly\n",
      " in\n",
      " the\n",
      " fields\n",
      " of\n",
      " algebra\n",
      " and\n",
      " tr\n",
      "ig\n",
      "on\n",
      "ometry\n",
      ".\n",
      " He\n",
      " is\n",
      " often\n",
      " considered\n",
      " the\n",
      " father\n",
      " of\n",
      " algebra\n",
      ".\n",
      "\n",
      "\n",
      "2\n",
      ".\n",
      " Al\n",
      "-\n",
      "R\n",
      "azi\n",
      " (\n",
      "8\n",
      "6\n",
      "5\n",
      " -\n",
      " \n",
      "9\n",
      "2\n",
      "5\n",
      ")\n",
      " -\n",
      " An\n",
      " influential\n",
      " Arab\n",
      " chem\n",
      "ist\n",
      ",\n",
      " physician\n",
      ",\n",
      " and\n",
      " philos\n",
      "opher\n",
      " who\n",
      " made\n",
      " important\n",
      " advance\n",
      "ments\n",
      " in\n",
      " chemistry\n",
      ",\n",
      " medicine\n",
      ",\n",
      " and\n",
      " pharm\n",
      "ac\n",
      "ology\n",
      ".\n",
      " He\n",
      " was\n",
      " known\n",
      " for\n",
      " his\n",
      " work\n",
      " on\n",
      " dist\n",
      "ill\n",
      "ation\n",
      " and\n",
      " the\n",
      " development\n",
      " of\n",
      " chemical\n",
      " comp\n",
      "ounds\n",
      ".\n",
      "\n",
      "\n",
      "3\n",
      ".\n",
      " Al\n",
      "-\n",
      "F\n",
      "ar\n",
      "abi\n",
      " (\n",
      "8\n",
      "7\n",
      "2\n",
      " or\n",
      " \n",
      "8\n",
      "7\n",
      "3\n",
      " -\n",
      " \n",
      "9\n",
      "5\n",
      "0\n",
      ")\n",
      " -\n",
      " A\n",
      " prominent\n",
      " Arab\n",
      " philos\n",
      "opher\n",
      ",\n",
      " log\n",
      "ician\n",
      ",\n",
      " and\n",
      " scientist\n",
      " who\n",
      " made\n",
      " significant\n",
      " contributions\n",
      " to\n",
      " various\n",
      " fields\n",
      ",\n",
      " including\n",
      " met\n",
      "aph\n",
      "ys\n",
      "ics\n",
      ",\n",
      " eth\n",
      "ics\n",
      ",\n",
      " political\n",
      " philosophy\n",
      ",\n",
      " and\n",
      " medicine\n",
      ".\n",
      " He\n",
      " was\n",
      " also\n",
      " known\n",
      " for\n",
      " his\n",
      " work\n",
      " in\n",
      " astr\n",
      "onomy\n",
      " and\n",
      " mathemat\n",
      "ics\n",
      ".\n",
      "\n",
      "\n",
      "4\n",
      ".\n",
      " Al\n",
      "-\n",
      "B\n",
      "ir\n",
      "uni\n",
      " (\n",
      "9\n",
      "7\n",
      "3\n",
      " -\n",
      " \n",
      "1\n",
      "0\n",
      "4\n",
      "8\n",
      ")\n",
      " -\n",
      " A\n",
      " poly\n",
      "math\n",
      " who\n",
      " made\n",
      " important\n",
      " contributions\n",
      " to\n",
      " various\n",
      " fields\n",
      " such\n",
      " as\n",
      " astr\n",
      "onomy\n",
      ",\n",
      " mathemat\n",
      "ics\n",
      ",\n",
      " ge\n",
      "ography\n",
      ",\n",
      " and\n",
      " philosophy\n",
      ".\n",
      " He\n",
      " is\n",
      " best\n",
      " known\n",
      " for\n",
      " his\n",
      " accurate\n",
      " astronom\n",
      "ical\n",
      " tables\n",
      " and\n",
      " his\n",
      " studies\n",
      " on\n",
      " the\n",
      " culture\n",
      " and\n",
      " religion\n",
      " of\n",
      " India\n",
      ".\n",
      "\n",
      "\n",
      "5\n",
      ".\n",
      " I\n",
      "bn\n",
      " S\n",
      "ina\n",
      " (\n",
      "Av\n",
      "ic\n",
      "en\n",
      "na\n",
      ",\n",
      " \n",
      "9\n",
      "8\n",
      "0\n",
      " -\n",
      " \n",
      "1\n",
      "0\n",
      "3\n",
      "7\n",
      ")\n",
      " -\n",
      " A\n",
      " prominent\n",
      " Pers\n",
      "ian\n",
      " physician\n",
      ",\n",
      " philos\n",
      "opher\n",
      ",\n",
      " and\n",
      " scientist\n",
      " who\n",
      " made\n",
      " significant\n",
      " contributions\n",
      " to\n",
      " various\n",
      " fields\n",
      ",\n",
      " including\n",
      " medicine\n",
      ",\n",
      " astr\n",
      "onomy\n",
      ",\n",
      " and\n",
      " psychology\n",
      ".\n",
      " His\n",
      " work\n",
      " \"\n",
      "The\n",
      " Can\n",
      "on\n",
      " of\n",
      " Medicine\n",
      "\"\n",
      " remained\n",
      " a\n",
      " standard\n",
      " medical\n",
      " text\n",
      " in\n",
      " the\n",
      " Islamic\n",
      " world\n",
      " and\n",
      " Europe\n",
      " for\n",
      " centuries\n",
      ".\n",
      "\n",
      "\n",
      "6\n",
      ".\n",
      " I\n",
      "bn\n",
      " al\n",
      "-\n",
      "H\n",
      "ay\n",
      "th\n",
      "am\n",
      " (\n",
      "9\n",
      "6\n",
      "5\n",
      " -\n",
      " \n",
      "1\n",
      "0\n",
      "4\n",
      "0\n",
      ")\n",
      " -\n",
      " An\n",
      " Arab\n",
      " mat\n",
      "hem\n",
      "atic\n",
      "ian\n",
      ",\n",
      " phys\n",
      "ic\n",
      "ist\n",
      ",\n",
      " and\n",
      " astronom\n",
      "er\n",
      " known\n",
      " for\n",
      " his\n",
      " pione\n",
      "ering\n",
      " work\n",
      " on\n",
      " opt\n",
      "ics\n",
      ",\n",
      " including\n",
      " the\n",
      " study\n",
      " of\n",
      " light\n",
      " and\n",
      " vision\n",
      ".\n",
      " His\n",
      " work\n",
      " laid\n",
      " the\n",
      " foundation\n",
      " for\n",
      " modern\n",
      " opt\n",
      "ics\n",
      " and\n",
      " contributed\n",
      " to\n",
      " the\n",
      " development\n",
      " of\n",
      " the\n",
      " camera\n",
      " obsc\n",
      "ura\n",
      " and\n",
      " the\n",
      " understanding\n",
      " of\n",
      " the\n",
      " human\n",
      " eye\n",
      ".\n",
      "\n",
      "\n",
      "7\n",
      ".\n",
      " Al\n",
      "-\n",
      "H\n",
      "ass\n",
      "ar\n",
      " (\n",
      "1\n",
      "0\n",
      "8\n",
      "9\n",
      " -\n",
      " \n",
      "1\n",
      "1\n",
      "5\n",
      "6\n",
      ")\n",
      " -\n",
      " An\n",
      " Arab\n",
      " mat\n",
      "hem\n",
      "atic\n",
      "ian\n",
      " who\n",
      " made\n",
      " significant\n",
      " contributions\n",
      " to\n",
      " algebra\n",
      ",\n",
      " particularly\n",
      " in\n",
      " solving\n",
      " cub\n",
      "ic\n",
      " and\n",
      " quart\n",
      "ic\n",
      " equations\n",
      ".\n",
      " His\n",
      " work\n",
      " laid\n",
      " the\n",
      " ground\n",
      "work\n",
      " for\n",
      " later\n",
      " mat\n",
      "hem\n",
      "atic\n",
      "ians\n",
      " such\n",
      " as\n",
      " F\n",
      "ib\n",
      "on\n",
      "acci\n",
      " and\n",
      " Leon\n",
      "ardo\n",
      " da\n",
      " Vin\n",
      "ci\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "These\n",
      " are\n",
      " just\n",
      " a\n",
      " few\n",
      " examples\n",
      " of\n",
      " Arab\n",
      " scientists\n",
      " who\n",
      " have\n",
      " made\n",
      " significant\n",
      " contributions\n",
      " to\n",
      " various\n",
      " fields\n",
      " throughout\n",
      " history\n",
      ".\n",
      "\n",
      " Yes\n",
      ",\n",
      " there\n",
      " are\n",
      " many\n",
      " modern\n",
      " Arab\n",
      " scientists\n",
      " who\n",
      " continue\n",
      " to\n",
      " make\n",
      " important\n",
      " contributions\n",
      " to\n",
      " various\n",
      " fields\n",
      ".\n",
      " Here\n",
      " are\n",
      " a\n",
      " few\n",
      " notable\n",
      " examples\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      ".\n",
      " Moh\n",
      "amed\n",
      " El\n",
      "-\n",
      "B\n",
      "az\n",
      " (\n",
      "B\n",
      "orn\n",
      " \n",
      "1\n",
      "9\n",
      "3\n",
      "6\n",
      ")\n",
      " -\n",
      " An\n",
      " Egyptian\n",
      " ge\n",
      "ologist\n",
      " and\n",
      " pale\n",
      "ont\n",
      "ologist\n",
      " known\n",
      " for\n",
      " his\n",
      " work\n",
      " on\n",
      " the\n",
      " ge\n",
      "ology\n",
      " and\n",
      " pale\n",
      "ont\n",
      "ology\n",
      " of\n",
      " Egypt\n",
      ",\n",
      " particularly\n",
      " in\n",
      " relation\n",
      " to\n",
      " the\n",
      " F\n",
      "oss\n",
      "il\n",
      " Record\n",
      " of\n",
      " the\n",
      " S\n",
      "ah\n",
      "ara\n",
      " Des\n",
      "ert\n",
      ".\n",
      " He\n",
      " has\n",
      " auth\n",
      "ored\n",
      " numerous\n",
      " books\n",
      " and\n",
      " research\n",
      " papers\n",
      " on\n",
      " the\n",
      " subject\n",
      ".\n",
      "\n",
      "\n",
      "2\n",
      ".\n",
      " Fer\n",
      "id\n",
      " Mur\n",
      "ad\n",
      " (\n",
      "B\n",
      "orn\n",
      " \n",
      "1\n",
      "9\n",
      "3\n",
      "2\n",
      ")\n",
      " -\n",
      " A\n",
      " Sy\n",
      "rian\n",
      "-\n",
      "American\n",
      " physician\n",
      " and\n",
      " scientist\n",
      " who\n",
      " shared\n",
      " the\n",
      " \n",
      "1\n",
      "9\n",
      "9\n",
      "8\n",
      " Nob\n",
      "el\n",
      " Prize\n",
      " in\n",
      " Phys\n",
      "i\n",
      "ology\n",
      " or\n",
      " Medicine\n",
      " for\n",
      " his\n",
      " work\n",
      " on\n",
      " nit\n",
      "ric\n",
      " ox\n",
      "ide\n",
      " as\n",
      " a\n",
      " sign\n",
      "aling\n",
      " mole\n",
      "cule\n",
      " in\n",
      " the\n",
      " card\n",
      "iov\n",
      "ascular\n",
      " system\n",
      ".\n",
      " He\n",
      " is\n",
      " a\n",
      " professor\n",
      " at\n",
      " Bay\n",
      "lor\n",
      " College\n",
      " of\n",
      " Medicine\n",
      " in\n",
      " Houston\n",
      ",\n",
      " Texas\n",
      ".\n",
      "\n",
      "\n",
      "3\n",
      ".\n",
      " H\n",
      "ania\n",
      " Far\n",
      "id\n",
      " (\n",
      "B\n",
      "orn\n",
      " \n",
      "1\n",
      "9\n",
      "7\n",
      "5\n",
      ")\n",
      " -\n",
      " An\n",
      " Egyptian\n",
      " computer\n",
      " scientist\n",
      " and\n",
      " expert\n",
      " in\n",
      " image\n",
      " fore\n",
      "ns\n",
      "ics\n",
      " who\n",
      " has\n",
      " made\n",
      " significant\n",
      " contributions\n",
      " to\n",
      " the\n",
      " field\n",
      " of\n",
      " digital\n",
      " image\n",
      " analysis\n",
      " and\n",
      " fore\n",
      "ns\n",
      "ic\n",
      " photo\n",
      " analysis\n",
      ".\n",
      " She\n",
      " is\n",
      " currently\n",
      " an\n",
      " Associ\n",
      "ate\n",
      " Professor\n",
      " at\n",
      " the\n",
      " University\n",
      " of\n",
      " Pennsylvania\n",
      ".\n",
      "\n",
      "\n",
      "4\n",
      ".\n",
      " Moh\n",
      "amed\n",
      " Ab\n",
      "el\n",
      "al\n",
      "ai\n",
      " (\n",
      "B\n",
      "orn\n",
      " \n",
      "1\n",
      "9\n",
      "6\n",
      "8\n",
      ")\n",
      " -\n",
      " A\n",
      " Tun\n",
      "is\n",
      "ian\n",
      " mat\n",
      "hem\n",
      "atic\n",
      "ian\n",
      " who\n",
      " has\n",
      " made\n",
      " significant\n",
      " contributions\n",
      " to\n",
      " the\n",
      " fields\n",
      " of\n",
      " differential\n",
      " equations\n",
      ",\n",
      " mathematical\n",
      " physics\n",
      ",\n",
      " and\n",
      " control\n",
      " theory\n",
      ".\n",
      " He\n",
      " is\n",
      " a\n",
      " professor\n",
      " at\n",
      " the\n",
      " University\n",
      " of\n",
      " S\n",
      "ous\n",
      "se\n",
      " in\n",
      " Tun\n",
      "is\n",
      "ia\n",
      ".\n",
      "\n",
      "\n",
      "5\n",
      ".\n",
      " Ah\n",
      "med\n",
      " H\n",
      ".\n",
      " Z\n",
      "ew\n",
      "ail\n",
      " (\n",
      "B\n",
      "orn\n",
      " \n",
      "1\n",
      "9\n",
      "4\n",
      "6\n",
      ")\n",
      " -\n",
      " An\n",
      " Egyptian\n",
      " chem\n",
      "ist\n",
      " known\n",
      " for\n",
      " his\n",
      " work\n",
      " on\n",
      " fem\n",
      "t\n",
      "oc\n",
      "hem\n",
      "istry\n",
      ",\n",
      " which\n",
      " involves\n",
      " studying\n",
      " chemical\n",
      " reactions\n",
      " at\n",
      " extremely\n",
      " fast\n",
      " time\n",
      " scales\n",
      ".\n",
      " In\n",
      " \n",
      "1\n",
      "9\n",
      "9\n",
      "9\n",
      ",\n",
      " he\n",
      " was\n",
      " awarded\n",
      " the\n",
      " Nob\n",
      "el\n",
      " Prize\n",
      " in\n",
      " Chem\n",
      "istry\n",
      " for\n",
      " his\n",
      " work\n",
      " on\n",
      " the\n",
      " development\n",
      " of\n",
      " fem\n",
      "tos\n",
      "ci\n",
      "ence\n",
      ".\n",
      " He\n",
      " is\n",
      " currently\n",
      " a\n",
      " professor\n",
      " at\n",
      " The\n",
      " California\n",
      " Institute\n",
      " of\n",
      " Technology\n",
      ".\n",
      "\n",
      "\n",
      "6\n",
      ".\n",
      " R\n",
      "ana\n",
      " D\n",
      "aj\n",
      "ani\n",
      " (\n",
      "B\n",
      "orn\n",
      " \n",
      "1\n",
      "9\n",
      "7\n",
      "3\n",
      ")\n",
      " -\n",
      " A\n",
      " Jordan\n",
      "ian\n",
      " psych\n",
      "ologist\n",
      " and\n",
      " neuro\n",
      "sc\n",
      "ient\n",
      "ist\n",
      " who\n",
      " has\n",
      " conducted\n",
      " research\n",
      " on\n",
      " the\n",
      " impact\n",
      " of\n",
      " cultural\n",
      " factors\n",
      " on\n",
      " cogn\n",
      "ition\n",
      " and\n",
      " behavior\n",
      ",\n",
      " particularly\n",
      " in\n",
      " relation\n",
      " to\n",
      " memory\n",
      " and\n",
      " learning\n",
      ".\n",
      " She\n",
      " is\n",
      " an\n",
      " associate\n",
      " professor\n",
      " at\n",
      " the\n",
      " Has\n",
      "hem\n",
      "ite\n",
      " University\n",
      " in\n",
      " Jordan\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "These\n",
      " are\n",
      " just\n",
      " a\n",
      " few\n",
      " examples\n",
      " of\n",
      " modern\n",
      " Arab\n",
      " scientists\n",
      " who\n",
      " continue\n",
      " to\n",
      " make\n",
      " significant\n",
      " contributions\n",
      " to\n",
      " various\n",
      " fields\n",
      ".\n",
      "\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = gr.Blocks()\n",
    "\n",
    "\n",
    "with block:\n",
    "\n",
    "    gr.Markdown(\"\"\"<h1><center> Jarvis </center></h1>\n",
    "    \"\"\")\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "    message = gr.Textbox(placeholder=\"Type here\")\n",
    "\n",
    "    state = gr.State()\n",
    "    with gr.Row():\n",
    "        top_k = gr.Slider(0.0,100.0, label=\"top_k\", value=40, info=\"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\")\n",
    "        top_p = gr.Slider(0.0,1.0, label=\"top_p\", value=0.9, info=\" Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\")\n",
    "        temp = gr.Slider(0.0,2.0, label=\"temperature\", value=0.8, info=\"The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)\")\n",
    "\n",
    "\n",
    "    submit = gr.Button(\"SEND\")\n",
    "\n",
    "    submit.click(chat, inputs=[message, state, top_k, top_p, temp], outputs=[chatbot, state])\n",
    "\n",
    "\n",
    "block.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472afe0-2e3c-4327-80b1-ef68483d0d76",
   "metadata": {},
   "source": [
    "# Through LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b471d4d5-8977-491d-90fb-184e076d15f6",
   "metadata": {},
   "source": [
    "### Loading a document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbef62a-8944-4ef5-9ae8-1c4a846f957e",
   "metadata": {},
   "source": [
    "### Web page"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f8b7388-ac24-449c-9cbe-a1414e64e09c",
   "metadata": {},
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://www.gutenberg.org/files/1727/1727-h/1727-h.htm\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c2cb7-8baf-401b-bf48-4a6ef0399afd",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a17c2a-7490-4c69-b689-d63c382f0380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('falcon-paper.pdf')\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0845c153-449b-454c-803b-1e17c656c2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c19de77-9d2f-436e-81d2-feb2e81eb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pages[:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a475b8f-e954-4feb-9dda-004597cbf9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Falcon Series of Open Language Models\\nThe Falcon LLM Team∗\\nEbtesam Almazrouei Hamza Alobeidli Ab'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "572b847a-9590-468a-9b63-730e6af086a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb7b2ba9-7905-4756-a0ea-2dc68d367e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9dabd70b-c974-4418-bc3d-b71e3509253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Falcon Series of Open Language Models\n",
      "The Falcon LLM Team∗\n",
      "Ebtesam Almazrouei Hamza Alobeidli Ab\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "# docs = r_splitter.split_documents(pages)\n",
    "paper_content = ' '.join([p.page_content for p in pages])\n",
    "print(paper_content[:100])\n",
    "docs = r_splitter.split_documents([Document(page_content=paper_content)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "47693cd9-645e-40fa-a07d-626d0a2076c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The Falcon Series of Open Language Models\\nThe Falcon LLM Team∗\\nEbtesam Almazrouei Hamza Alobeidli Abdulaziz Alshamsi Alessandro Cappelli\\nRuxandra Cojocaru Mérouane Debbah Etienne Go ffinet Daniel Hesslow Julien Launay\\nQuentin Malartic Daniele Mazzotta Badreddine Noune Baptiste Pannier Guilherme Penedo\\nTechnology Innovation Institute, Abu Dhabi\\nhttps://huggingface.co/tiiuae/\\nAbstract\\nWe introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-'\n",
      "page_content='all. By open-sourcing artificial intelligence research and models, we can foster a broader and more\\ndiverse community, and benefit from vibrant collaborative e fforts to improve the safety and reliability\\nof large language models. We hope the Falcon series can be a small step towards this vision.\\n39'\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])\n",
    "print(docs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822019fd-1f55-43b8-886e-f2593cfe140d",
   "metadata": {},
   "source": [
    "### embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "315cdc37-e546-4ae6-8b3a-6f4a244ef198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings model: sentence-transformers/all-MiniLM-L12-v2 ...\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "# EMBEDDING_MODEL_NAME = 'OrdalieTech/Solon-embeddings-large-0.1'\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "print(f\"Loading embeddings model: {EMBEDDING_MODEL_NAME} ...\")\n",
    "\n",
    "embedding_model = LangchainEmbedding(HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    encode_kwargs = {\"normalize_embeddings\": False}\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6e449-12ec-4b02-9b2f-2cdd60d1ecfc",
   "metadata": {},
   "source": [
    "### llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71db6446-3cf5-4bff-a784-ca11b713664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama2\",\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "073d953f-cae5-4f9e-925c-703ba1406629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Artificial intelligence (AI) has a rich and varied history that spans several decades. Here is a brief overview of some of the key milestones in the development of AI:\n",
      "\n",
      "1. 1950s: The Dartmouth Conference: The field of AI was founded at a conference held at Dartmouth College in 1956. Attendees included computer scientists, mathematicians, and cognitive scientists who were interested in exploring the possibilities of creating machines that could simulate human intelligence.\n",
      "2. 1951: The Turing Test: British mathematician Alan Turing proposed a test to measure a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. The Turing Test has since become a benchmark for measuring the success of AI systems.\n",
      "3. 1956: The First AI Program: Computer scientist John McCarthy created the first AI program, called the Logical Theorist, which was designed to reason and solve problems using logical deduction.\n",
      "4. 1960s: Rule-Based Expert Systems: The development of rule-based expert systems, which used a set of rules to reason and make decisions, marked a significant milestone in the history of AI. These systems were widely used in industries such as banking and healthcare.\n",
      "5. 1970s: Machine Learning: Machine learning, which enables machines to learn from data without being explicitly programmed, emerged as a major area of research in AI. This led to the development of algorithms such as decision trees and neural networks.\n",
      "6. 1980s: Expert Systems: The development of expert systems, which were designed to mimic the decision-making abilities of human experts, reached its peak in the 1980s. These systems were widely used in industries such as banking and healthcare.\n",
      "7. 1990s: AI Winter: Despite the progress that had been made in AI research, the field experienced a decline in funding and interest in the 1990s, which became known as the \"AI winter.\"\n",
      "8. 2000s: Resurgence of AI: The resurgence of AI in the 2000s was driven by advances in computing power, data storage, and machine learning algorithms. This led to the development of applications such as speech recognition, image recognition, and natural language processing.\n",
      "9. Present Day: Today, AI is a rapidly growing field, with applications in areas such as self-driving cars, healthcare, finance, and education. The rise of deep learning and neural networks has enabled AI systems to perform tasks such as image and speech recognition, natural language processing, and decision making.\n",
      "\n",
      "Some of the key figures in the history of AI include:\n",
      "\n",
      "1. Alan Turing: British mathematician and computer scientist who proposed the Turing Test and developed the concept of the universal Turing machine.\n",
      "2. John McCarthy: Computer scientist who coined the term \"artificial intelligence\" and created the first AI program, the Logical Theorist.\n",
      "3. Marvin Minsky: American computer scientist and cognitive scientist who made significant contributions to the field of artificial neural networks.\n",
      "4. Seymour Papert: Canadian mathematician and cognitive scientist who developed the concept of \"perceptrons,\" a type of neural network.\n",
      "5. Ray Kurzweil: American inventor and computer scientist who has written extensively on the potential of AI to transform society.\n",
      "6. Nick Bostrom: Swedish philosopher and director of the Future of Humanity Institute, who has written extensively on the ethical implications of advanced AI systems.\n",
      "\n",
      "These are just a few examples of the many individuals who have contributed to the field of AI over the years. The history of AI is a rich and complex one, and it continues to evolve and expand today."
     ]
    }
   ],
   "source": [
    "_ = llm(\"Tell me about the history of AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0514a-bba8-4eb4-8229-af0249ff5a7d",
   "metadata": {},
   "source": [
    "### llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37b6c1e4-60fd-479c-b792-15358612ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "document = Document(text=paper_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ef7f51a-4f50-44d8-ad6b-2fc3af1a8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=embedding_model\n",
    ")\n",
    "index = VectorStoreIndex.from_documents([document],\n",
    "                                        service_context=service_context)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ec67320-1ec2-410a-9bac-6d50944a5700",
   "metadata": {},
   "source": [
    "from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "\n",
    "def build_sentence_window_index(\n",
    "    document, llm, vector_store, embed_model=\"local:BAAI/bge-small-en-v1.5\"\n",
    "):\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=3,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    sentence_index = VectorStoreIndex.from_documents(\n",
    "        [document], service_context=sentence_context, storage_context=storage_context\n",
    "    )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "def get_sentence_window_query_engine(\n",
    "    sentence_index,\n",
    "    similarity_top_k=6,\n",
    "    rerank_top_n=2,\n",
    "):\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "    )\n",
    "    return sentence_window_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d33cecd-b78b-4fa7-988a-31877866c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2969e49-1acd-49ab-934a-8109911566cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Falcon series of models, we focused primarily on hardware scalability across three axes: performance, data, and hardware. Large-scale training requires thousands of hardware accelerators to work efficiently in unison; making the best use of these accelerators requires principled distributed training methods. Methods that are able to best run efficiently and leverage large-scale compute are often the ones that gain the most traction in the community, as best evidenced by the Transformer architecture itself. Furthermore, it is difficult to find architectural improvements that significantly improve the task performance of models, compared to the impact of data for instance. Accordingly, we focus architectural decisions not on improving task performance, but on improving hardware scalability and throughput."
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What about Hardware scalability?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "25cceead-bb42-43dc-be94-7fdb06ea9e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1b91e95f4d4b01b64819dde91f062a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9cf9cb5305476db90392f300f4e6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdef27619f8d42a099c0e0304c138a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1130ba5b90144d85985c4098cafa7848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415b187eaa4d477a9b392bffef9b0aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726538dc028a491294ba71a139000f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98c50cecaed4849b455d153ea3e637b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad3af6ee0f749f2a59f6df5612162d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0627915879b347ef97e9002c7d4e01b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da32116c2274374adf238207867892b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f165e2b84bb340899a19b49817479886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8214ab17f24bdf80d6548505d3cde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d98a3c92924a3bb937308fc2dcfc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3534952606c343e2a5ea2dfdc215f987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "docs = r_splitter.split_documents([Document(page_content=paper_content)])\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7efca580-f292-4dfd-8cf4-f57611d6ec3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware scalability. Large-scale training requires thousands of hardware accelerators to work\n",
      "efficiently in unison; making the best use of these accelerators requires in turn principled distributed\n",
      "training methods (Shoeybi et al., 2019). Methods that are able to best run e fficiently and leverage\n",
      "large-scale compute are often the ones that gain the most traction in the community (Hooker, 2021),\n",
      "as best evidenced by the Transformer architecture itself (Vaswani et al., 2017). Furthermore, it\n"
     ]
    }
   ],
   "source": [
    "query = \"What about Hardware scalability?\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4be9fddf-34a5-46b2-8e65-3cbabb40ab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "Based on the provided context, it seems that hardware scalability is a crucial aspect of the Falcon series of models. The authors emphasize the importance of leveraging large-scale compute to achieve efficient and effective training methods. They also mention that the best use of hardware accelerators requires principled distributed training methods, which are able to run efficiently and leverage large-scale compute.\n",
      "\n",
      "In particular, the authors highlight the challenges of improving task performance through architectural improvements alone, as the sustained increase in the scale of large language models can be difficult to achieve without significant investments in hardware scalability and throughput. To address this challenge, they focus on improving hardware scalability and throughput, rather than task performance directly.\n",
      "\n",
      "Furthermore, the authors mention the adoption of tweaks such as a revised multiquery attention scheme to improve inference scalability. This suggests that they are continuously exploring ways to optimize their models for large-scale distributed training on cloud infrastructure with Gigatron.\n",
      "\n",
      "Overall, it appears that hardware scalability is a key consideration in the design and optimization of the Falcon series of models.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What about Hardware scalability?',\n",
       " 'result': 'Based on the provided context, it seems that hardware scalability is a crucial aspect of the Falcon series of models. The authors emphasize the importance of leveraging large-scale compute to achieve efficient and effective training methods. They also mention that the best use of hardware accelerators requires principled distributed training methods, which are able to run efficiently and leverage large-scale compute.\\n\\nIn particular, the authors highlight the challenges of improving task performance through architectural improvements alone, as the sustained increase in the scale of large language models can be difficult to achieve without significant investments in hardware scalability and throughput. To address this challenge, they focus on improving hardware scalability and throughput, rather than task performance directly.\\n\\nFurthermore, the authors mention the adoption of tweaks such as a revised multiquery attention scheme to improve inference scalability. This suggests that they are continuously exploring ways to optimize their models for large-scale distributed training on cloud infrastructure with Gigatron.\\n\\nOverall, it appears that hardware scalability is a key consideration in the design and optimization of the Falcon series of models.'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qachain=RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever(), verbose=True)\n",
    "qachain({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078ad05-7720-4526-a344-74c4daeaeb43",
   "metadata": {},
   "source": [
    "# Chain it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df785e3-8dc4-491e-b30d-e6144be4ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@shrinath.suresh/implementing-streaming-chatbot-with-langchain-callbacks-a-step-by-step-guide-a527a7d65b8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2b83cc4-df1f-483f-8dfe-9468030b7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import Any\n",
    "from queue import Queue, Empty\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b5def3bc-0158-4b7a-a961-e52ceb7f04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Queue()\n",
    "job_done = object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cba1faa7-3f68-4f0f-ab32-876b5baac865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueueCallback(BaseCallbackHandler):\n",
    "    \"\"\"Callback handler for streaming LLM responses to a queue.\"\"\"\n",
    "\n",
    "    def __init__(self, q):\n",
    "        self.q = q\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        self.q.put(token)\n",
    "\n",
    "    def on_llm_end(self, *args, **kwargs: Any) -> None:\n",
    "        return self.q.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c1e83285-79e1-4fd3-b5d3-12d5eba96352",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [QueueCallback(q)]\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a8eb2265-513e-4679-bec3-55234bf49c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(question):\n",
    "  def task():\n",
    "    response = llm(question)\n",
    "    q.put(job_done)\n",
    "  \n",
    "  t = Thread(target=task)\n",
    "  t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bf6a8741-9a19-4cec-b21c-1a5ff45e4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.callbacks = callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "76fd82a7-099d-496c-8ab0-4934933c5467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  hello\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        question = history[-1][0]\n",
    "        print(\"Question: \", question)\n",
    "        history[-1][1] = \"\"\n",
    "        answer(question=question)\n",
    "        while True:\n",
    "          try:\n",
    "            next_token = q.get(True, timeout=1)\n",
    "            if next_token is job_done:\n",
    "              break\n",
    "            history[-1][1] += next_token\n",
    "            yield history\n",
    "          except Empty:\n",
    "            continue\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf56c66-6e27-413f-8f93-41fd4de1ca5c",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7a5f8508-a1b5-4c0a-b387-b1db64310049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "62a19b39-ee80-4944-8fbf-42587a53dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an smart and helpful assistant of an AI researcher. Given the following context, answer the question:\n",
    "Context:{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f346c209-4513-47ee-a023-220508e45769",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5539d757-8dea-47f3-a039-4e53c0cd751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a47c3b99-849e-4e43-b9ba-266c6220f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_rag(question):\n",
    "  def task():\n",
    "    response = chain.invoke(question)\n",
    "    q.put(job_done)\n",
    "  \n",
    "  t = Thread(target=task)\n",
    "  t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7d5b0495-2fb6-4605-ac68-7b7ecf1292a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  what is hardware scalability ?\n",
      "Question:  what is special about Falcon?\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        question = history[-1][0]\n",
    "        print(\"Question: \", question)\n",
    "        history[-1][1] = \"\"\n",
    "        answer_rag(question=question)\n",
    "        while True:\n",
    "          try:\n",
    "            next_token = q.get(True, timeout=1)\n",
    "            if next_token is job_done:\n",
    "              break\n",
    "            history[-1][1] += next_token\n",
    "            yield history\n",
    "          except Empty:\n",
    "            continue\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a41be-128b-4312-a1f9-3719d2e48bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "what is hardware scalability ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff7487-8ce4-4a57-acb7-e53ff55819e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
